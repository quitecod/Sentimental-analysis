{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB_sentiment_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quitecod/Sentimental-analysis/blob/main/IMDB_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hLdF1oit06A"
      },
      "source": [
        "# Sentiment analysis of IMDB reviews\n",
        "We will start by importing the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzJqzHv-LceW"
      },
      "source": [
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54C8Oca0LlVR"
      },
      "source": [
        "import tensorflow.keras as keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npBPD8k_PwRJ"
      },
      "source": [
        "# Importing the data files\n",
        "After importing the necessary libraries now we will read the data files we have two data files here\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOvZ5uMyOtTF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "276e350c-4581-4aac-daf8-3414dead8ee7"
      },
      "source": [
        "imdb_reviews=pd.read_csv(\"/content/imdb_reviews.csv\")\n",
        "test_reviews=pd.read_csv(\"/content/test_reviews.csv\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-cf41544dda0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimdb_reviews\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/imdb_reviews.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_reviews\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/test_reviews.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/imdb_reviews.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4a_Zs-8WhcH"
      },
      "source": [
        "first data file contains the imdb reviews and their corresponding sentiments which can be either positive or negative, we are going to use this file as our training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI4qnZOTPFqx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "f6edbb11-b2ce-4e2d-e498-884b1d5c3bc3"
      },
      "source": [
        "imdb_reviews.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b6d9c6ff1979>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimdb_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'imdb_reviews' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rNGROl5XYrL"
      },
      "source": [
        "the second file is also similar to the first file but we are going to use it as the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGqg6S8OXVmV"
      },
      "source": [
        "test_reviews.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOfnoRhGXogV"
      },
      "source": [
        "# Preprocessing the data\n",
        "We can not pass the string data to our model directly, so we need to transform the string data into integer format.For this we can map each distinct word as a distinct integer for eg.{'this':14 , 'the':1}.We already have a file that contains the mapping from words to integers so we are going to load that file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf5XTq_hAxL6"
      },
      "source": [
        "word_index=pd.read_csv(\"/content/word_indexes.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE47q3yibEIM"
      },
      "source": [
        "The word index file contains mapping from words to integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbkABN8jA_Ye",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9e7ba619-7176-4e44-e0a7-155ca10ed731"
      },
      "source": [
        "word_index.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Words</th>\n",
              "      <th>Indexes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tsukino</td>\n",
              "      <td>52009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>nunnery</td>\n",
              "      <td>52010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sonja</td>\n",
              "      <td>16819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>vani</td>\n",
              "      <td>63954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>woods</td>\n",
              "      <td>1411</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Words  Indexes\n",
              "0  tsukino    52009\n",
              "1  nunnery    52010\n",
              "2    sonja    16819\n",
              "3     vani    63954\n",
              "4    woods     1411"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZYF6pDzbNCe"
      },
      "source": [
        "Next we are going to convert the word_index dataframe into a python dictionary so that we can use it for converting our reviews from string to integer format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuiMcF5XD65R"
      },
      "source": [
        "word_index=dict(zip(word_index.Words,word_index.Indexes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsScw2ilFqap"
      },
      "source": [
        "word_index[\"<PAD>\"]=0\n",
        "word_index[\"<START\"]=1\n",
        "word_index[\"<UNK>\"]=2\n",
        "word_index[\"<UNUSED>\"]=3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVEL7jQ6bjbV"
      },
      "source": [
        "Now we define a function review_encoder that encodes the reviews into integer format according to the mapping specified by word_index file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nay4o8dSAqbu"
      },
      "source": [
        "def review_encoder(text):\n",
        "  arr=[word_index[word] for word in text]\n",
        "  return arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3mS11cVcGhU"
      },
      "source": [
        "We split the reviews from their corresponding sentiments so that we can preprocess the reviews and sentiments separately and then later pass it to our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUhde9eLE5H3"
      },
      "source": [
        "train_data,train_labels=imdb_reviews['Reviews'],imdb_reviews['Sentiment']\n",
        "test_data, test_labels=test_reviews['Reviews'],test_reviews['Sentiment']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBWDVwCNdBJP"
      },
      "source": [
        "Before transforming the reviews as integers we need to tokenize or split the review on the basis of whitespaces\n",
        "For eg.the string \"The movie was wonderful\" becomes [\"The\" , \"movie\" , \"was\" , \"wonderful\" ]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frWCqvycL-w5"
      },
      "source": [
        "train_data=train_data.apply(lambda review:review.split())\n",
        "test_data=test_data.apply(lambda review:review.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GCM_74ve7OZ"
      },
      "source": [
        "Since we have tokenized the reviews now we can apply the review_encoder function to each review and transform the reviews into integer format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJLwZH1OMPJM"
      },
      "source": [
        "train_data=train_data.apply(review_encoder)\n",
        "test_data=test_data.apply(review_encoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8kdTaoOfj8d"
      },
      "source": [
        "After transforming, our reviews are going to look like this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3M_klO4MhFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df45cc5c-eb63-4a45-c3d9-86550578669c"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, ...\n",
              "1    [1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463,...\n",
              "2    [1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5...\n",
              "3    [1, 4, 2, 2, 33, 2804, 4, 2040, 432, 111, 153,...\n",
              "4    [1, 249, 1323, 7, 61, 113, 10, 10, 13, 1637, 1...\n",
              "Name: Reviews, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEUf78Aif02A"
      },
      "source": [
        "We also need to encode the sentiments and we are labeling the positive sentiment as 1 and negative sentiment as 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p01DiByEJ7fw"
      },
      "source": [
        "def encode_sentiments(x):\n",
        "  if x=='positive':\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "train_labels=train_labels.apply(encode_sentiments)\n",
        "test_labels=test_labels.apply(encode_sentiments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPOP1mPd1mrh"
      },
      "source": [
        "Before giving the review as an input to the model we need to perform following preprocessing steps:\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "*   The length of each review should be made equal for the model to be working correctly.\n",
        "\n",
        "*  We have chosen the length of each review to be 500. \n",
        "*     If the review is longer than 500 words we are going to cut the extra part of the review.\n",
        "\n",
        "\n",
        "*       If the review is contains less than 500 words we are going to pad the review with zeros to increase its length to 500.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHEVR3TbPE_1"
      },
      "source": [
        "train_data=keras.preprocessing.sequence.pad_sequences(train_data,value=word_index[\"<PAD>\"],padding='post',maxlen=500)\n",
        "test_data=keras.preprocessing.sequence.pad_sequences(test_data,value=word_index[\"<PAD>\"],padding='post',maxlen=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjL1Q8kG2YXV"
      },
      "source": [
        "#Building the model\n",
        "Our model is a neural network and it consits of the following layers : \n",
        "\n",
        "1.   one word embedding layer which creates word embeddings of length 16 from integer encoded review.\n",
        "2.  second layer is global average pooling layer which is used to prevent overfitting by reducing the number of parameters.\n",
        "\n",
        "1.   then a dense layer which has 16 hidden units and uses relu as activation function\n",
        "2.  the final layer is the output layer which uses sigmoid as activation function \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1WDSfAIPs31"
      },
      "source": [
        "model=keras.Sequential([keras.layers.Embedding(10000,16,input_length=500),\n",
        "                        keras.layers.GlobalAveragePooling1D(),\n",
        "                        keras.layers.Dense(16,activation='relu'),\n",
        "                        keras.layers.Dense(1,activation='sigmoid')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pK_1v8v3hBZ"
      },
      "source": [
        "#compiling the model\n",
        "\n",
        "\n",
        "1.   Adam is used as optimization function for our model.\n",
        "2.   Binary cross entropy loss function is used as loss function for the model.\n",
        "\n",
        "1.   Accuracy is used as the metric for evaluating the model.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPsQuds5QYud"
      },
      "source": [
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wJsxdk24H_g"
      },
      "source": [
        "In the next step we are going to train the model on our downloaded IMDB dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxUCLcRJQmBB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dff0e7b-4544-48ac-fb87-4108ab34552b"
      },
      "source": [
        "#training the model\n",
        "history=model.fit(train_data,train_labels,epochs=30,batch_size=512,validation_data=(test_data,test_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "49/49 [==============================] - 2s 35ms/step - loss: 0.6928 - accuracy: 0.5190 - val_loss: 0.6909 - val_accuracy: 0.5599\n",
            "Epoch 2/30\n",
            "49/49 [==============================] - 2s 31ms/step - loss: 0.6891 - accuracy: 0.5882 - val_loss: 0.6829 - val_accuracy: 0.7026\n",
            "Epoch 3/30\n",
            "49/49 [==============================] - 1s 31ms/step - loss: 0.6776 - accuracy: 0.6778 - val_loss: 0.6639 - val_accuracy: 0.6885\n",
            "Epoch 4/30\n",
            "49/49 [==============================] - 1s 30ms/step - loss: 0.6515 - accuracy: 0.7399 - val_loss: 0.6276 - val_accuracy: 0.7748\n",
            "Epoch 5/30\n",
            "49/49 [==============================] - 2s 31ms/step - loss: 0.6076 - accuracy: 0.7931 - val_loss: 0.5792 - val_accuracy: 0.7976\n",
            "Epoch 6/30\n",
            "49/49 [==============================] - 1s 30ms/step - loss: 0.5547 - accuracy: 0.8149 - val_loss: 0.5286 - val_accuracy: 0.8072\n",
            "Epoch 7/30\n",
            "49/49 [==============================] - 1s 30ms/step - loss: 0.4946 - accuracy: 0.8377 - val_loss: 0.4778 - val_accuracy: 0.8342\n",
            "Epoch 8/30\n",
            "49/49 [==============================] - 1s 31ms/step - loss: 0.4444 - accuracy: 0.8564 - val_loss: 0.4381 - val_accuracy: 0.8378\n",
            "Epoch 9/30\n",
            "49/49 [==============================] - 2s 31ms/step - loss: 0.4007 - accuracy: 0.8697 - val_loss: 0.4091 - val_accuracy: 0.8464\n",
            "Epoch 10/30\n",
            "49/49 [==============================] - 2s 34ms/step - loss: 0.3669 - accuracy: 0.8758 - val_loss: 0.3801 - val_accuracy: 0.8596\n",
            "Epoch 11/30\n",
            "49/49 [==============================] - 2s 31ms/step - loss: 0.3336 - accuracy: 0.8855 - val_loss: 0.3602 - val_accuracy: 0.8650\n",
            "Epoch 12/30\n",
            "49/49 [==============================] - 2s 31ms/step - loss: 0.3127 - accuracy: 0.8925 - val_loss: 0.3450 - val_accuracy: 0.8690\n",
            "Epoch 13/30\n",
            "49/49 [==============================] - 2s 31ms/step - loss: 0.2945 - accuracy: 0.8967 - val_loss: 0.3324 - val_accuracy: 0.8723\n",
            "Epoch 14/30\n",
            "49/49 [==============================] - 1s 30ms/step - loss: 0.2787 - accuracy: 0.9030 - val_loss: 0.3229 - val_accuracy: 0.8748\n",
            "Epoch 15/30\n",
            "49/49 [==============================] - 2s 31ms/step - loss: 0.2678 - accuracy: 0.9042 - val_loss: 0.3150 - val_accuracy: 0.8774\n",
            "Epoch 16/30\n",
            "49/49 [==============================] - 1s 31ms/step - loss: 0.2543 - accuracy: 0.9116 - val_loss: 0.3098 - val_accuracy: 0.8776\n",
            "Epoch 17/30\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.2441 - accuracy: 0.9160 - val_loss: 0.3051 - val_accuracy: 0.8794\n",
            "Epoch 18/30\n",
            "49/49 [==============================] - 2s 33ms/step - loss: 0.2341 - accuracy: 0.9183 - val_loss: 0.2990 - val_accuracy: 0.8822\n",
            "Epoch 19/30\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.2218 - accuracy: 0.9231 - val_loss: 0.2960 - val_accuracy: 0.8826\n",
            "Epoch 20/30\n",
            "49/49 [==============================] - 2s 33ms/step - loss: 0.2183 - accuracy: 0.9254 - val_loss: 0.2930 - val_accuracy: 0.8840\n",
            "Epoch 21/30\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.2068 - accuracy: 0.9277 - val_loss: 0.2905 - val_accuracy: 0.8845\n",
            "Epoch 22/30\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.1963 - accuracy: 0.9336 - val_loss: 0.2895 - val_accuracy: 0.8852\n",
            "Epoch 23/30\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 0.1982 - accuracy: 0.9297 - val_loss: 0.2879 - val_accuracy: 0.8861\n",
            "Epoch 24/30\n",
            "49/49 [==============================] - 1s 30ms/step - loss: 0.1871 - accuracy: 0.9373 - val_loss: 0.2866 - val_accuracy: 0.8864\n",
            "Epoch 25/30\n",
            "49/49 [==============================] - 1s 31ms/step - loss: 0.1821 - accuracy: 0.9378 - val_loss: 0.2888 - val_accuracy: 0.8848\n",
            "Epoch 26/30\n",
            "49/49 [==============================] - 1s 31ms/step - loss: 0.1802 - accuracy: 0.9389 - val_loss: 0.2865 - val_accuracy: 0.8861\n",
            "Epoch 27/30\n",
            "49/49 [==============================] - 2s 31ms/step - loss: 0.1736 - accuracy: 0.9423 - val_loss: 0.2858 - val_accuracy: 0.8858\n",
            "Epoch 28/30\n",
            "49/49 [==============================] - 1s 31ms/step - loss: 0.1696 - accuracy: 0.9424 - val_loss: 0.2865 - val_accuracy: 0.8861\n",
            "Epoch 29/30\n",
            "49/49 [==============================] - 1s 30ms/step - loss: 0.1634 - accuracy: 0.9440 - val_loss: 0.2871 - val_accuracy: 0.8858\n",
            "Epoch 30/30\n",
            "49/49 [==============================] - 1s 31ms/step - loss: 0.1592 - accuracy: 0.9485 - val_loss: 0.2879 - val_accuracy: 0.8858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FQWLj-i4dEK"
      },
      "source": [
        "Now we will be evaluating the loss and accuracy of our model on testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHLzEJI-RCWF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f3c84cf-fc02-4d9c-c25d-b8ca15b3e95a"
      },
      "source": [
        "loss,accuracy=model.evaluate(test_data,test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 1s 2ms/step - loss: 0.2879 - accuracy: 0.8858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhtQ2vt44tvd"
      },
      "source": [
        "As we can see our model is giving an accuracy of 88.58% on the testing data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWH8VZPmisqr"
      },
      "source": [
        "Now we are going to take a random review from our test dataset and check wether our model produces correct output or not"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiqpYZu7ipTa",
        "outputId": "aff57399-fb6b-426a-820b-86a143e5752f"
      },
      "source": [
        "index=np.random.randint(1,1000)\n",
        "user_review=test_reviews.loc[index]\n",
        "print(user_review)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reviews      <START is a really well done 6 hour drama abou...\n",
            "Sentiment                                             positive\n",
            "Name: 452, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml5oNL-dj-Ix"
      },
      "source": [
        "As we can see the sentiment for the above review is positive, now we are going to take the integer format of this particular review which we already have in our preprocessed test data and then give it as an input to our model to check the prediction of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLJGmSkej9Jl",
        "outputId": "6cc03304-d42e-4247-adcd-6f96da3c5ee1"
      },
      "source": [
        "user_review=test_data[index]\n",
        "user_review=np.array([user_review])\n",
        "if (model.predict(user_review)>0.5).astype(\"int32\"):\n",
        "  print(\"positive sentiment\")\n",
        "else:\n",
        "  print(\"negative sentiment\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive sentiment\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBfalQWOmg66"
      },
      "source": [
        "As we can see our model is now able to predict the sentiment of the review."
      ]
    }
  ]
}